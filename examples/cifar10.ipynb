{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "cifar10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7VZmo59y1Gy"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zG9Mfmsqw6-f",
    "outputId": "dc24fafd-d5f9-40ec-c2bd-9185e11d940d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!git clone https://github.com/yeyinthtoon/tf2-resmlp.git\n",
    "%cd tf2-resmlp/\n",
    "!pip install ."
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'tf2-resmlp'...\n",
      "remote: Enumerating objects: 50, done.\u001b[K\n",
      "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
      "remote: Total 50 (delta 16), reused 43 (delta 13), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (50/50), done.\n",
      "/content/tf2-resmlp\n",
      "Branch 'enchant-add-example' set up to track remote branch 'enchant-add-example' from 'origin'.\n",
      "Switched to a new branch 'enchant-add-example'\n",
      "Processing /content/tf2-resmlp\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting einops>=0.3\n",
      "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: res-mlp-tensorflow\n",
      "  Building wheel for res-mlp-tensorflow (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for res-mlp-tensorflow: filename=res_mlp_tensorflow-0.0.1-py3-none-any.whl size=5294 sha256=7344167386bbbe047d4fe4e3b421dc23855a5b77429c7b51db9d0da0164fd13b\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/87/5e/ad6410557ecab7244a8801ce009c604ff7592f71d7509b4dc3\n",
      "Successfully built res-mlp-tensorflow\n",
      "Installing collected packages: einops, res-mlp-tensorflow\n",
      "Successfully installed einops-0.3.0 res-mlp-tensorflow-0.0.1\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LDcuj7CeytwC"
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from resmlp import ResMlp12"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muR3nOOezfek"
   },
   "source": [
    "### TPU setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UX5h3HN7zery",
    "outputId": "a4c26cd7-b59f-40df-aece-84c347cb40fb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "try:\n",
    "    tpu = (\n",
    "        tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    )  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print(\"Running on TPU \", tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.49.133.194:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.49.133.194:8470\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.49.133.194:8470\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9sEDgaN2c6a"
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "44EuofD_zj65"
   },
   "source": [
    "configs = {\n",
    "    \"seed\": 112,\n",
    "    \"data_augmentation\": True,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 256,\n",
    "    \"drop_path_rate\": 0.3,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"num_classes\": 10,\n",
    "    \"input_image_size\": [224, 224],\n",
    "}"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlE-9kbIzQ-1"
   },
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vwlAEUu0y-ym",
    "outputId": "f6cecba0-45a3-4118-dae2-00938622f347",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_test -= x_train_mean"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7lK1qGzzUbo"
   },
   "source": [
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def set_shape(image, label):\n",
    "    image = tf.image.resize(image, configs[\"input_image_size\"])\n",
    "    image.set_shape(configs[\"input_image_size\"] + [3])\n",
    "    label.set_shape(\n",
    "        [\n",
    "            1,\n",
    "        ]\n",
    "    )\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(10000, seed=configs[\"seed\"])\n",
    "if configs[\"data_augmentation\"]:\n",
    "    train_ds = train_ds.map(augment)\n",
    "train_ds = train_ds.map(set_shape)\n",
    "train_ds = train_ds.batch(configs[\"batch_size\"], drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.map(set_shape)\n",
    "test_ds = test_ds.batch(configs[\"batch_size\"], drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26kzYxJT2iL7"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jz8rpMHszX3-"
   },
   "source": [
    "tf.keras.backend.clear_session()\n",
    "with strategy.scope():\n",
    "    model = ResMlp12(\n",
    "        input_shape=configs[\"input_image_size\"] + [3],\n",
    "        num_classes=configs[\"num_classes\"],\n",
    "        dropout_rate=configs[\"dropout_rate\"],\n",
    "        drop_path_rate=configs[\"drop_path_rate\"],\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mydrezsl56E6",
    "outputId": "f3359726-c17c-4992-e097-ca49fc460df8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"resmlp12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_embedding_conv_1 (Conv2D) (None, 14, 14, 384)  295296      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rearrange (Rearrange)           (None, 196, 384)     0           patch_embedding_conv_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_0_affine_1 (DiagonalAffin (None, 196, 384)     768         rearrange[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 384, 196)     0           block_0_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_0_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 196, 384)     0           block_0_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_0_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_0_drop_path_1 (PerSampleD (None, 196, 384)     0           block_0_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_0_add_1 (Add)             (None, 196, 384)     0           rearrange[0][0]                  \n",
      "                                                                 block_0_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_0_affine_3 (DiagonalAffin (None, 196, 384)     768         block_0_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_0_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_0_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_0_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_0_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_0_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_0_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_0_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_0_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_0_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_0_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_0_affine_4 (DiagonalAffin (None, 196, 384)     384         block_0_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_0_drop_path_2 (PerSampleD (None, 196, 384)     0           block_0_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_0_add_2 (Add)             (None, 196, 384)     0           block_0_add_1[0][0]              \n",
      "                                                                 block_0_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_affine_1 (DiagonalAffin (None, 196, 384)     768         block_0_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 384, 196)     0           block_1_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 196, 384)     0           block_1_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_drop_path_1 (PerSampleD (None, 196, 384)     0           block_1_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1_add_1 (Add)             (None, 196, 384)     0           block_0_add_2[0][0]              \n",
      "                                                                 block_1_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_affine_3 (DiagonalAffin (None, 196, 384)     768         block_1_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_1_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_1_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_1_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_1_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_1_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_affine_4 (DiagonalAffin (None, 196, 384)     384         block_1_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_drop_path_2 (PerSampleD (None, 196, 384)     0           block_1_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1_add_2 (Add)             (None, 196, 384)     0           block_1_add_1[0][0]              \n",
      "                                                                 block_1_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_affine_1 (DiagonalAffin (None, 196, 384)     768         block_1_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_4 (TFOpL (None, 384, 196)     0           block_2_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_2_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_5 (TFOpL (None, 196, 384)     0           block_2_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_2_drop_path_1 (PerSampleD (None, 196, 384)     0           block_2_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add_1 (Add)             (None, 196, 384)     0           block_1_add_2[0][0]              \n",
      "                                                                 block_2_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_affine_3 (DiagonalAffin (None, 196, 384)     768         block_2_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_2_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_2_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_2_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_2_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_2_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_2_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_2_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_affine_4 (DiagonalAffin (None, 196, 384)     384         block_2_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_2_drop_path_2 (PerSampleD (None, 196, 384)     0           block_2_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add_2 (Add)             (None, 196, 384)     0           block_2_add_1[0][0]              \n",
      "                                                                 block_2_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_affine_1 (DiagonalAffin (None, 196, 384)     768         block_2_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_6 (TFOpL (None, 384, 196)     0           block_3_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_3_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_7 (TFOpL (None, 196, 384)     0           block_3_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_3_drop_path_1 (PerSampleD (None, 196, 384)     0           block_3_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_3_add_1 (Add)             (None, 196, 384)     0           block_2_add_2[0][0]              \n",
      "                                                                 block_3_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_affine_3 (DiagonalAffin (None, 196, 384)     768         block_3_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_3_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_3_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_3_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_3_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_3_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_3_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_3_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_affine_4 (DiagonalAffin (None, 196, 384)     384         block_3_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_3_drop_path_2 (PerSampleD (None, 196, 384)     0           block_3_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_3_add_2 (Add)             (None, 196, 384)     0           block_3_add_1[0][0]              \n",
      "                                                                 block_3_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_affine_1 (DiagonalAffin (None, 196, 384)     768         block_3_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_8 (TFOpL (None, 384, 196)     0           block_4_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_4_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_9 (TFOpL (None, 196, 384)     0           block_4_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_4_drop_path_1 (PerSampleD (None, 196, 384)     0           block_4_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add_1 (Add)             (None, 196, 384)     0           block_3_add_2[0][0]              \n",
      "                                                                 block_4_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_affine_3 (DiagonalAffin (None, 196, 384)     768         block_4_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_4_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_4_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_4_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_4_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_4_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_4_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_4_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_affine_4 (DiagonalAffin (None, 196, 384)     384         block_4_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_4_drop_path_2 (PerSampleD (None, 196, 384)     0           block_4_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add_2 (Add)             (None, 196, 384)     0           block_4_add_1[0][0]              \n",
      "                                                                 block_4_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_affine_1 (DiagonalAffin (None, 196, 384)     768         block_4_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_10 (TFOp (None, 384, 196)     0           block_5_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_5_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_11 (TFOp (None, 196, 384)     0           block_5_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_5_drop_path_1 (PerSampleD (None, 196, 384)     0           block_5_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add_1 (Add)             (None, 196, 384)     0           block_4_add_2[0][0]              \n",
      "                                                                 block_5_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_affine_3 (DiagonalAffin (None, 196, 384)     768         block_5_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_5_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_5_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_5_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_5_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_5_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_5_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_5_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_5_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_affine_4 (DiagonalAffin (None, 196, 384)     384         block_5_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_5_drop_path_2 (PerSampleD (None, 196, 384)     0           block_5_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add_2 (Add)             (None, 196, 384)     0           block_5_add_1[0][0]              \n",
      "                                                                 block_5_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_affine_1 (DiagonalAffin (None, 196, 384)     768         block_5_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_12 (TFOp (None, 384, 196)     0           block_6_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_6_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_13 (TFOp (None, 196, 384)     0           block_6_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_6_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_6_drop_path_1 (PerSampleD (None, 196, 384)     0           block_6_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_6_add_1 (Add)             (None, 196, 384)     0           block_5_add_2[0][0]              \n",
      "                                                                 block_6_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_affine_3 (DiagonalAffin (None, 196, 384)     768         block_6_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_6_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_6_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_6_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_6_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_6_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_6_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_6_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_6_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_affine_4 (DiagonalAffin (None, 196, 384)     384         block_6_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_6_drop_path_2 (PerSampleD (None, 196, 384)     0           block_6_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_6_add_2 (Add)             (None, 196, 384)     0           block_6_add_1[0][0]              \n",
      "                                                                 block_6_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_affine_1 (DiagonalAffin (None, 196, 384)     768         block_6_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_14 (TFOp (None, 384, 196)     0           block_7_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_7_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_15 (TFOp (None, 196, 384)     0           block_7_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_7_drop_path_1 (PerSampleD (None, 196, 384)     0           block_7_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add_1 (Add)             (None, 196, 384)     0           block_6_add_2[0][0]              \n",
      "                                                                 block_7_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_affine_3 (DiagonalAffin (None, 196, 384)     768         block_7_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_7_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_7_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_7_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_7_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_7_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_7_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_7_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_7_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_affine_4 (DiagonalAffin (None, 196, 384)     384         block_7_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_7_drop_path_2 (PerSampleD (None, 196, 384)     0           block_7_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add_2 (Add)             (None, 196, 384)     0           block_7_add_1[0][0]              \n",
      "                                                                 block_7_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_affine_1 (DiagonalAffin (None, 196, 384)     768         block_7_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_16 (TFOp (None, 384, 196)     0           block_8_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_8_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_17 (TFOp (None, 196, 384)     0           block_8_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_8_drop_path_1 (PerSampleD (None, 196, 384)     0           block_8_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add_1 (Add)             (None, 196, 384)     0           block_7_add_2[0][0]              \n",
      "                                                                 block_8_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_affine_3 (DiagonalAffin (None, 196, 384)     768         block_8_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_8_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_8_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_8_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_8_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_8_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_8_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_8_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_8_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_affine_4 (DiagonalAffin (None, 196, 384)     384         block_8_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_8_drop_path_2 (PerSampleD (None, 196, 384)     0           block_8_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add_2 (Add)             (None, 196, 384)     0           block_8_add_1[0][0]              \n",
      "                                                                 block_8_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_affine_1 (DiagonalAffin (None, 196, 384)     768         block_8_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_18 (TFOp (None, 384, 196)     0           block_9_affine_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_9_dense_1 (Dense)         (None, 384, 196)     38612       tf.compat.v1.transpose_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_19 (TFOp (None, 196, 384)     0           block_9_dense_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_affine_2 (DiagonalAffin (None, 196, 384)     384         tf.compat.v1.transpose_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_9_drop_path_1 (PerSampleD (None, 196, 384)     0           block_9_affine_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add_1 (Add)             (None, 196, 384)     0           block_8_add_2[0][0]              \n",
      "                                                                 block_9_drop_path_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_affine_3 (DiagonalAffin (None, 196, 384)     768         block_9_add_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_9_mlp_dense_1 (Dense)     (None, 196, 1536)    591360      block_9_affine_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_9_mlp_act_1 (Activation)  (None, 196, 1536)    0           block_9_mlp_dense_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_mlp_dropout_1 (Dropout) (None, 196, 1536)    0           block_9_mlp_act_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_mlp_dense_2 (Dense)     (None, 196, 384)     590208      block_9_mlp_dropout_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_9_mlp_dropout_2 (Dropout) (None, 196, 384)     0           block_9_mlp_dense_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_affine_4 (DiagonalAffin (None, 196, 384)     384         block_9_mlp_dropout_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_9_drop_path_2 (PerSampleD (None, 196, 384)     0           block_9_affine_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add_2 (Add)             (None, 196, 384)     0           block_9_add_1[0][0]              \n",
      "                                                                 block_9_drop_path_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_10_affine_1 (DiagonalAffi (None, 196, 384)     768         block_9_add_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_20 (TFOp (None, 384, 196)     0           block_10_affine_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_10_dense_1 (Dense)        (None, 384, 196)     38612       tf.compat.v1.transpose_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_21 (TFOp (None, 196, 384)     0           block_10_dense_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_10_affine_2 (DiagonalAffi (None, 196, 384)     384         tf.compat.v1.transpose_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_10_drop_path_1 (PerSample (None, 196, 384)     0           block_10_affine_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_10_add_1 (Add)            (None, 196, 384)     0           block_9_add_2[0][0]              \n",
      "                                                                 block_10_drop_path_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_affine_3 (DiagonalAffi (None, 196, 384)     768         block_10_add_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_10_mlp_dense_1 (Dense)    (None, 196, 1536)    591360      block_10_affine_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_10_mlp_act_1 (Activation) (None, 196, 1536)    0           block_10_mlp_dense_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_mlp_dropout_1 (Dropout (None, 196, 1536)    0           block_10_mlp_act_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_mlp_dense_2 (Dense)    (None, 196, 384)     590208      block_10_mlp_dropout_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_10_mlp_dropout_2 (Dropout (None, 196, 384)     0           block_10_mlp_dense_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_affine_4 (DiagonalAffi (None, 196, 384)     384         block_10_mlp_dropout_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_10_drop_path_2 (PerSample (None, 196, 384)     0           block_10_affine_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_10_add_2 (Add)            (None, 196, 384)     0           block_10_add_1[0][0]             \n",
      "                                                                 block_10_drop_path_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_affine_1 (DiagonalAffi (None, 196, 384)     768         block_10_add_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_22 (TFOp (None, 384, 196)     0           block_11_affine_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_11_dense_1 (Dense)        (None, 384, 196)     38612       tf.compat.v1.transpose_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_23 (TFOp (None, 196, 384)     0           block_11_dense_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_affine_2 (DiagonalAffi (None, 196, 384)     384         tf.compat.v1.transpose_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block_11_drop_path_1 (PerSample (None, 196, 384)     0           block_11_affine_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add_1 (Add)            (None, 196, 384)     0           block_10_add_2[0][0]             \n",
      "                                                                 block_11_drop_path_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_affine_3 (DiagonalAffi (None, 196, 384)     768         block_11_add_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_11_mlp_dense_1 (Dense)    (None, 196, 1536)    591360      block_11_affine_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_11_mlp_act_1 (Activation) (None, 196, 1536)    0           block_11_mlp_dense_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_mlp_dropout_1 (Dropout (None, 196, 1536)    0           block_11_mlp_act_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_mlp_dense_2 (Dense)    (None, 196, 384)     590208      block_11_mlp_dropout_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_11_mlp_dropout_2 (Dropout (None, 196, 384)     0           block_11_mlp_dense_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_affine_4 (DiagonalAffi (None, 196, 384)     384         block_11_mlp_dropout_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_11_drop_path_2 (PerSample (None, 196, 384)     0           block_11_affine_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add_2 (Add)            (None, 196, 384)     0           block_11_add_1[0][0]             \n",
      "                                                                 block_11_drop_path_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "feature_affine (DiagonalAffine) (None, 196, 384)     768         block_11_add_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduce (Reduce)                 (None, 384)          0           feature_affine[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 10)           3850        reduce[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,969,722\n",
      "Trainable params: 14,969,722\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XutOqqwd3O6x"
   },
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V4JXfU793UVc",
    "outputId": "68107859-996a-4d6a-b02a-814cc66584ac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=configs[\"epochs\"],\n",
    "    validation_data=(test_ds),\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 59s 120ms/step - loss: 1.8074 - accuracy: 0.3406 - val_loss: 1.4649 - val_accuracy: 0.4724\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 1.4310 - accuracy: 0.4870 - val_loss: 1.2728 - val_accuracy: 0.5303\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 1.2669 - accuracy: 0.5476 - val_loss: 1.1454 - val_accuracy: 0.5817\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 1.1619 - accuracy: 0.5846 - val_loss: 1.0282 - val_accuracy: 0.6346\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 1.0979 - accuracy: 0.6078 - val_loss: 1.0055 - val_accuracy: 0.6386\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 1.0430 - accuracy: 0.6290 - val_loss: 0.9465 - val_accuracy: 0.6591\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.9981 - accuracy: 0.6467 - val_loss: 0.9187 - val_accuracy: 0.6679\n",
      "Epoch 8/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.9557 - accuracy: 0.6609 - val_loss: 0.8821 - val_accuracy: 0.6873\n",
      "Epoch 9/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.9279 - accuracy: 0.6699 - val_loss: 0.8961 - val_accuracy: 0.6795\n",
      "Epoch 10/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.8977 - accuracy: 0.6806 - val_loss: 0.8197 - val_accuracy: 0.7039\n",
      "Epoch 11/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.8497 - accuracy: 0.6999 - val_loss: 0.7997 - val_accuracy: 0.7147\n",
      "Epoch 12/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.8091 - accuracy: 0.7094 - val_loss: 0.7808 - val_accuracy: 0.7181\n",
      "Epoch 13/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.7897 - accuracy: 0.7212 - val_loss: 0.7485 - val_accuracy: 0.7345\n",
      "Epoch 14/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.7597 - accuracy: 0.7286 - val_loss: 0.7315 - val_accuracy: 0.7354\n",
      "Epoch 15/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.7337 - accuracy: 0.7417 - val_loss: 0.7266 - val_accuracy: 0.7443\n",
      "Epoch 16/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.7142 - accuracy: 0.7474 - val_loss: 0.7413 - val_accuracy: 0.7403\n",
      "Epoch 17/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.6940 - accuracy: 0.7550 - val_loss: 0.6919 - val_accuracy: 0.7559\n",
      "Epoch 18/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.6777 - accuracy: 0.7599 - val_loss: 0.7077 - val_accuracy: 0.7549\n",
      "Epoch 19/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.6576 - accuracy: 0.7681 - val_loss: 0.7020 - val_accuracy: 0.7594\n",
      "Epoch 20/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.6454 - accuracy: 0.7706 - val_loss: 0.6610 - val_accuracy: 0.7725\n",
      "Epoch 21/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.6271 - accuracy: 0.7776 - val_loss: 0.6624 - val_accuracy: 0.7722\n",
      "Epoch 22/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.6139 - accuracy: 0.7820 - val_loss: 0.6935 - val_accuracy: 0.7569\n",
      "Epoch 23/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.5986 - accuracy: 0.7876 - val_loss: 0.6686 - val_accuracy: 0.7731\n",
      "Epoch 24/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.5817 - accuracy: 0.7941 - val_loss: 0.6533 - val_accuracy: 0.7700\n",
      "Epoch 25/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.5738 - accuracy: 0.7955 - val_loss: 0.6426 - val_accuracy: 0.7798\n",
      "Epoch 26/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.5596 - accuracy: 0.8020 - val_loss: 0.6440 - val_accuracy: 0.7804\n",
      "Epoch 27/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.5521 - accuracy: 0.8035 - val_loss: 0.6332 - val_accuracy: 0.7845\n",
      "Epoch 28/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.5439 - accuracy: 0.8083 - val_loss: 0.6334 - val_accuracy: 0.7830\n",
      "Epoch 29/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.5240 - accuracy: 0.8150 - val_loss: 0.6504 - val_accuracy: 0.7794\n",
      "Epoch 30/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.5173 - accuracy: 0.8152 - val_loss: 0.6020 - val_accuracy: 0.7898\n",
      "Epoch 31/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.5032 - accuracy: 0.8204 - val_loss: 0.6308 - val_accuracy: 0.7857\n",
      "Epoch 32/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.4928 - accuracy: 0.8253 - val_loss: 0.6278 - val_accuracy: 0.7924\n",
      "Epoch 33/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4851 - accuracy: 0.8259 - val_loss: 0.6107 - val_accuracy: 0.7939\n",
      "Epoch 34/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4718 - accuracy: 0.8328 - val_loss: 0.6272 - val_accuracy: 0.7925\n",
      "Epoch 35/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4620 - accuracy: 0.8336 - val_loss: 0.6415 - val_accuracy: 0.7929\n",
      "Epoch 36/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4567 - accuracy: 0.8374 - val_loss: 0.6275 - val_accuracy: 0.7918\n",
      "Epoch 37/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4476 - accuracy: 0.8407 - val_loss: 0.6232 - val_accuracy: 0.7940\n",
      "Epoch 38/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4308 - accuracy: 0.8466 - val_loss: 0.6333 - val_accuracy: 0.7913\n",
      "Epoch 39/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4302 - accuracy: 0.8484 - val_loss: 0.6251 - val_accuracy: 0.7964\n",
      "Epoch 40/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.4159 - accuracy: 0.8508 - val_loss: 0.6140 - val_accuracy: 0.8039\n",
      "Epoch 41/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.4109 - accuracy: 0.8534 - val_loss: 0.6331 - val_accuracy: 0.7978\n",
      "Epoch 42/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.4128 - accuracy: 0.8523 - val_loss: 0.6199 - val_accuracy: 0.8006\n",
      "Epoch 43/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3966 - accuracy: 0.8598 - val_loss: 0.6371 - val_accuracy: 0.7963\n",
      "Epoch 44/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3894 - accuracy: 0.8603 - val_loss: 0.6434 - val_accuracy: 0.7953\n",
      "Epoch 45/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3790 - accuracy: 0.8661 - val_loss: 0.6220 - val_accuracy: 0.8005\n",
      "Epoch 46/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.3770 - accuracy: 0.8653 - val_loss: 0.6308 - val_accuracy: 0.7997\n",
      "Epoch 47/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.3686 - accuracy: 0.8686 - val_loss: 0.6214 - val_accuracy: 0.8041\n",
      "Epoch 48/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3590 - accuracy: 0.8709 - val_loss: 0.6423 - val_accuracy: 0.8025\n",
      "Epoch 49/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3576 - accuracy: 0.8721 - val_loss: 0.6216 - val_accuracy: 0.8055\n",
      "Epoch 50/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.3492 - accuracy: 0.8743 - val_loss: 0.6310 - val_accuracy: 0.8009\n",
      "Epoch 51/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.3439 - accuracy: 0.8753 - val_loss: 0.6450 - val_accuracy: 0.7991\n",
      "Epoch 52/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3349 - accuracy: 0.8812 - val_loss: 0.6650 - val_accuracy: 0.8004\n",
      "Epoch 53/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3338 - accuracy: 0.8807 - val_loss: 0.6299 - val_accuracy: 0.8006\n",
      "Epoch 54/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.3306 - accuracy: 0.8807 - val_loss: 0.6233 - val_accuracy: 0.8079\n",
      "Epoch 55/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3175 - accuracy: 0.8872 - val_loss: 0.6577 - val_accuracy: 0.8033\n",
      "Epoch 56/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.3140 - accuracy: 0.8861 - val_loss: 0.6710 - val_accuracy: 0.8028\n",
      "Epoch 57/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.3118 - accuracy: 0.8878 - val_loss: 0.6562 - val_accuracy: 0.8064\n",
      "Epoch 58/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3109 - accuracy: 0.8889 - val_loss: 0.6790 - val_accuracy: 0.8037\n",
      "Epoch 59/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.3071 - accuracy: 0.8919 - val_loss: 0.6333 - val_accuracy: 0.8094\n",
      "Epoch 60/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.2955 - accuracy: 0.8960 - val_loss: 0.6555 - val_accuracy: 0.8123\n",
      "Epoch 61/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2912 - accuracy: 0.8961 - val_loss: 0.6575 - val_accuracy: 0.8104\n",
      "Epoch 62/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2936 - accuracy: 0.8942 - val_loss: 0.6465 - val_accuracy: 0.8075\n",
      "Epoch 63/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.2810 - accuracy: 0.9007 - val_loss: 0.6653 - val_accuracy: 0.8053\n",
      "Epoch 64/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.2774 - accuracy: 0.9008 - val_loss: 0.6528 - val_accuracy: 0.8114\n",
      "Epoch 65/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2796 - accuracy: 0.8999 - val_loss: 0.6509 - val_accuracy: 0.8075\n",
      "Epoch 66/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2715 - accuracy: 0.9038 - val_loss: 0.7009 - val_accuracy: 0.8010\n",
      "Epoch 67/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2758 - accuracy: 0.9012 - val_loss: 0.6618 - val_accuracy: 0.8098\n",
      "Epoch 68/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2605 - accuracy: 0.9082 - val_loss: 0.6872 - val_accuracy: 0.8063\n",
      "Epoch 69/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2590 - accuracy: 0.9086 - val_loss: 0.7014 - val_accuracy: 0.8037\n",
      "Epoch 70/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.2693 - accuracy: 0.9050 - val_loss: 0.6890 - val_accuracy: 0.8081\n",
      "Epoch 71/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2507 - accuracy: 0.9102 - val_loss: 0.7025 - val_accuracy: 0.8082\n",
      "Epoch 72/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2528 - accuracy: 0.9110 - val_loss: 0.6818 - val_accuracy: 0.8086\n",
      "Epoch 73/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.2471 - accuracy: 0.9117 - val_loss: 0.6886 - val_accuracy: 0.8077\n",
      "Epoch 74/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2478 - accuracy: 0.9119 - val_loss: 0.7018 - val_accuracy: 0.8059\n",
      "Epoch 75/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2400 - accuracy: 0.9156 - val_loss: 0.7172 - val_accuracy: 0.8057\n",
      "Epoch 76/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2461 - accuracy: 0.9140 - val_loss: 0.6589 - val_accuracy: 0.8104\n",
      "Epoch 77/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2329 - accuracy: 0.9177 - val_loss: 0.7168 - val_accuracy: 0.8084\n",
      "Epoch 78/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2313 - accuracy: 0.9191 - val_loss: 0.6899 - val_accuracy: 0.8127\n",
      "Epoch 79/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2280 - accuracy: 0.9200 - val_loss: 0.6984 - val_accuracy: 0.8117\n",
      "Epoch 80/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2272 - accuracy: 0.9186 - val_loss: 0.7195 - val_accuracy: 0.8112\n",
      "Epoch 81/200\n",
      "Learning rate:  0.001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.2240 - accuracy: 0.9211 - val_loss: 0.6905 - val_accuracy: 0.8122\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.1763 - accuracy: 0.9391 - val_loss: 0.6916 - val_accuracy: 0.8210\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1482 - accuracy: 0.9470 - val_loss: 0.7032 - val_accuracy: 0.8221\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1359 - accuracy: 0.9524 - val_loss: 0.7207 - val_accuracy: 0.8198\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.1351 - accuracy: 0.9528 - val_loss: 0.7377 - val_accuracy: 0.8233\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1293 - accuracy: 0.9550 - val_loss: 0.7460 - val_accuracy: 0.8223\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1240 - accuracy: 0.9564 - val_loss: 0.7515 - val_accuracy: 0.8205\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1219 - accuracy: 0.9584 - val_loss: 0.7474 - val_accuracy: 0.8220\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1148 - accuracy: 0.9590 - val_loss: 0.7716 - val_accuracy: 0.8215\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1136 - accuracy: 0.9598 - val_loss: 0.7757 - val_accuracy: 0.8222\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1156 - accuracy: 0.9597 - val_loss: 0.7667 - val_accuracy: 0.8223\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1125 - accuracy: 0.9609 - val_loss: 0.7637 - val_accuracy: 0.8241\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1089 - accuracy: 0.9630 - val_loss: 0.7755 - val_accuracy: 0.8217\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.1043 - accuracy: 0.9638 - val_loss: 0.7910 - val_accuracy: 0.8214\n",
      "Epoch 95/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1054 - accuracy: 0.9623 - val_loss: 0.7960 - val_accuracy: 0.8217\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.1046 - accuracy: 0.9634 - val_loss: 0.7901 - val_accuracy: 0.8244\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1032 - accuracy: 0.9635 - val_loss: 0.8005 - val_accuracy: 0.8233\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.1018 - accuracy: 0.9643 - val_loss: 0.7947 - val_accuracy: 0.8254\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.1011 - accuracy: 0.9650 - val_loss: 0.8103 - val_accuracy: 0.8223\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0958 - accuracy: 0.9660 - val_loss: 0.8169 - val_accuracy: 0.8230\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0948 - accuracy: 0.9676 - val_loss: 0.8236 - val_accuracy: 0.8250\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0932 - accuracy: 0.9683 - val_loss: 0.8272 - val_accuracy: 0.8220\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0936 - accuracy: 0.9680 - val_loss: 0.8302 - val_accuracy: 0.8237\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0938 - accuracy: 0.9671 - val_loss: 0.8328 - val_accuracy: 0.8220\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0960 - accuracy: 0.9672 - val_loss: 0.8273 - val_accuracy: 0.8241\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.0944 - accuracy: 0.9671 - val_loss: 0.8347 - val_accuracy: 0.8244\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0931 - accuracy: 0.9679 - val_loss: 0.8366 - val_accuracy: 0.8253\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0905 - accuracy: 0.9692 - val_loss: 0.8364 - val_accuracy: 0.8253\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.0900 - accuracy: 0.9689 - val_loss: 0.8471 - val_accuracy: 0.8227\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0912 - accuracy: 0.9691 - val_loss: 0.8406 - val_accuracy: 0.8237\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0872 - accuracy: 0.9705 - val_loss: 0.8485 - val_accuracy: 0.8248\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0870 - accuracy: 0.9694 - val_loss: 0.8505 - val_accuracy: 0.8259\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0883 - accuracy: 0.9689 - val_loss: 0.8436 - val_accuracy: 0.8248\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.0854 - accuracy: 0.9701 - val_loss: 0.8527 - val_accuracy: 0.8235\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0843 - accuracy: 0.9706 - val_loss: 0.8621 - val_accuracy: 0.8239\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0841 - accuracy: 0.9705 - val_loss: 0.8483 - val_accuracy: 0.8255\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.0839 - accuracy: 0.9718 - val_loss: 0.8512 - val_accuracy: 0.8229\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0846 - accuracy: 0.9704 - val_loss: 0.8674 - val_accuracy: 0.8241\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0808 - accuracy: 0.9726 - val_loss: 0.8577 - val_accuracy: 0.8261\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0814 - accuracy: 0.9717 - val_loss: 0.8626 - val_accuracy: 0.8259\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0001\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0819 - accuracy: 0.9718 - val_loss: 0.8689 - val_accuracy: 0.8244\n",
      "Epoch 122/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0772 - accuracy: 0.9735 - val_loss: 0.8658 - val_accuracy: 0.8249\n",
      "Epoch 123/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0782 - accuracy: 0.9725 - val_loss: 0.8649 - val_accuracy: 0.8259\n",
      "Epoch 124/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0756 - accuracy: 0.9739 - val_loss: 0.8649 - val_accuracy: 0.8258\n",
      "Epoch 125/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0782 - accuracy: 0.9733 - val_loss: 0.8627 - val_accuracy: 0.8258\n",
      "Epoch 126/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.0780 - accuracy: 0.9734 - val_loss: 0.8636 - val_accuracy: 0.8258\n",
      "Epoch 127/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0777 - accuracy: 0.9731 - val_loss: 0.8691 - val_accuracy: 0.8253\n",
      "Epoch 128/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0729 - accuracy: 0.9751 - val_loss: 0.8707 - val_accuracy: 0.8247\n",
      "Epoch 129/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0738 - accuracy: 0.9747 - val_loss: 0.8740 - val_accuracy: 0.8255\n",
      "Epoch 130/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0740 - accuracy: 0.9744 - val_loss: 0.8742 - val_accuracy: 0.8247\n",
      "Epoch 131/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0745 - accuracy: 0.9744 - val_loss: 0.8714 - val_accuracy: 0.8253\n",
      "Epoch 132/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0708 - accuracy: 0.9755 - val_loss: 0.8741 - val_accuracy: 0.8252\n",
      "Epoch 133/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0712 - accuracy: 0.9750 - val_loss: 0.8773 - val_accuracy: 0.8251\n",
      "Epoch 134/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.0740 - accuracy: 0.9748 - val_loss: 0.8796 - val_accuracy: 0.8244\n",
      "Epoch 135/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0727 - accuracy: 0.9753 - val_loss: 0.8803 - val_accuracy: 0.8259\n",
      "Epoch 136/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0755 - accuracy: 0.9745 - val_loss: 0.8786 - val_accuracy: 0.8251\n",
      "Epoch 137/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0729 - accuracy: 0.9755 - val_loss: 0.8773 - val_accuracy: 0.8261\n",
      "Epoch 138/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0740 - accuracy: 0.9745 - val_loss: 0.8789 - val_accuracy: 0.8258\n",
      "Epoch 139/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0721 - accuracy: 0.9748 - val_loss: 0.8825 - val_accuracy: 0.8247\n",
      "Epoch 140/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0710 - accuracy: 0.9756 - val_loss: 0.8831 - val_accuracy: 0.8249\n",
      "Epoch 141/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.0752 - accuracy: 0.9742 - val_loss: 0.8799 - val_accuracy: 0.8246\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0744 - accuracy: 0.9743 - val_loss: 0.8822 - val_accuracy: 0.8246\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0742 - accuracy: 0.9743 - val_loss: 0.8821 - val_accuracy: 0.8249\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0731 - accuracy: 0.9750 - val_loss: 0.8802 - val_accuracy: 0.8255\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.0714 - accuracy: 0.9754 - val_loss: 0.8809 - val_accuracy: 0.8249\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.0742 - accuracy: 0.9744 - val_loss: 0.8824 - val_accuracy: 0.8252\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0708 - accuracy: 0.9759 - val_loss: 0.8805 - val_accuracy: 0.8257\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0738 - accuracy: 0.9744 - val_loss: 0.8841 - val_accuracy: 0.8262\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.0773 - accuracy: 0.9739 - val_loss: 0.8795 - val_accuracy: 0.8258\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0736 - accuracy: 0.9751 - val_loss: 0.8790 - val_accuracy: 0.8255\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0706 - accuracy: 0.9759 - val_loss: 0.8819 - val_accuracy: 0.8261\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.0739 - accuracy: 0.9755 - val_loss: 0.8813 - val_accuracy: 0.8250\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0718 - accuracy: 0.9762 - val_loss: 0.8817 - val_accuracy: 0.8250\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0742 - accuracy: 0.9755 - val_loss: 0.8871 - val_accuracy: 0.8247\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0718 - accuracy: 0.9762 - val_loss: 0.8829 - val_accuracy: 0.8243\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0723 - accuracy: 0.9749 - val_loss: 0.8818 - val_accuracy: 0.8254\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0713 - accuracy: 0.9747 - val_loss: 0.8839 - val_accuracy: 0.8252\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0719 - accuracy: 0.9754 - val_loss: 0.8835 - val_accuracy: 0.8241\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0703 - accuracy: 0.9762 - val_loss: 0.8853 - val_accuracy: 0.8246\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0690 - accuracy: 0.9760 - val_loss: 0.8875 - val_accuracy: 0.8243\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0733 - accuracy: 0.9744 - val_loss: 0.8902 - val_accuracy: 0.8249\n",
      "Epoch 162/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0726 - accuracy: 0.9747 - val_loss: 0.8901 - val_accuracy: 0.8251\n",
      "Epoch 163/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0706 - accuracy: 0.9762 - val_loss: 0.8902 - val_accuracy: 0.8250\n",
      "Epoch 164/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0701 - accuracy: 0.9752 - val_loss: 0.8905 - val_accuracy: 0.8251\n",
      "Epoch 165/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 18s 92ms/step - loss: 0.0685 - accuracy: 0.9768 - val_loss: 0.8906 - val_accuracy: 0.8251\n",
      "Epoch 166/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0701 - accuracy: 0.9769 - val_loss: 0.8911 - val_accuracy: 0.8248\n",
      "Epoch 167/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0690 - accuracy: 0.9763 - val_loss: 0.8914 - val_accuracy: 0.8247\n",
      "Epoch 168/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0702 - accuracy: 0.9748 - val_loss: 0.8912 - val_accuracy: 0.8249\n",
      "Epoch 169/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0733 - accuracy: 0.9747 - val_loss: 0.8911 - val_accuracy: 0.8247\n",
      "Epoch 170/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0688 - accuracy: 0.9762 - val_loss: 0.8912 - val_accuracy: 0.8246\n",
      "Epoch 171/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0691 - accuracy: 0.9762 - val_loss: 0.8912 - val_accuracy: 0.8248\n",
      "Epoch 172/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0685 - accuracy: 0.9758 - val_loss: 0.8915 - val_accuracy: 0.8250\n",
      "Epoch 173/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.0704 - accuracy: 0.9763 - val_loss: 0.8915 - val_accuracy: 0.8253\n",
      "Epoch 174/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0715 - accuracy: 0.9752 - val_loss: 0.8916 - val_accuracy: 0.8250\n",
      "Epoch 175/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0696 - accuracy: 0.9768 - val_loss: 0.8911 - val_accuracy: 0.8253\n",
      "Epoch 176/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0695 - accuracy: 0.9765 - val_loss: 0.8909 - val_accuracy: 0.8252\n",
      "Epoch 177/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.8911 - val_accuracy: 0.8250\n",
      "Epoch 178/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0710 - accuracy: 0.9758 - val_loss: 0.8910 - val_accuracy: 0.8251\n",
      "Epoch 179/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0689 - accuracy: 0.9765 - val_loss: 0.8911 - val_accuracy: 0.8253\n",
      "Epoch 180/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0706 - accuracy: 0.9758 - val_loss: 0.8907 - val_accuracy: 0.8253\n",
      "Epoch 181/200\n",
      "Learning rate:  1e-06\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0662 - accuracy: 0.9764 - val_loss: 0.8906 - val_accuracy: 0.8253\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0704 - accuracy: 0.9752 - val_loss: 0.8907 - val_accuracy: 0.8254\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0698 - accuracy: 0.9755 - val_loss: 0.8906 - val_accuracy: 0.8251\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0703 - accuracy: 0.9757 - val_loss: 0.8903 - val_accuracy: 0.8251\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0714 - accuracy: 0.9762 - val_loss: 0.8901 - val_accuracy: 0.8253\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 88ms/step - loss: 0.0720 - accuracy: 0.9754 - val_loss: 0.8899 - val_accuracy: 0.8254\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0732 - accuracy: 0.9747 - val_loss: 0.8899 - val_accuracy: 0.8255\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0678 - accuracy: 0.9761 - val_loss: 0.8902 - val_accuracy: 0.8254\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0709 - accuracy: 0.9754 - val_loss: 0.8902 - val_accuracy: 0.8255\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0688 - accuracy: 0.9770 - val_loss: 0.8901 - val_accuracy: 0.8254\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0751 - accuracy: 0.9749 - val_loss: 0.8901 - val_accuracy: 0.8253\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0696 - accuracy: 0.9757 - val_loss: 0.8900 - val_accuracy: 0.8256\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0705 - accuracy: 0.9762 - val_loss: 0.8901 - val_accuracy: 0.8258\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0686 - accuracy: 0.9770 - val_loss: 0.8902 - val_accuracy: 0.8259\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0708 - accuracy: 0.9770 - val_loss: 0.8904 - val_accuracy: 0.8258\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 18s 91ms/step - loss: 0.0672 - accuracy: 0.9769 - val_loss: 0.8905 - val_accuracy: 0.8257\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 18s 90ms/step - loss: 0.0710 - accuracy: 0.9757 - val_loss: 0.8903 - val_accuracy: 0.8255\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0692 - accuracy: 0.9761 - val_loss: 0.8902 - val_accuracy: 0.8255\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 17s 89ms/step - loss: 0.0734 - accuracy: 0.9747 - val_loss: 0.8901 - val_accuracy: 0.8257\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-07\n",
      "195/195 [==============================] - 18s 89ms/step - loss: 0.0682 - accuracy: 0.9768 - val_loss: 0.8901 - val_accuracy: 0.8258\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}
